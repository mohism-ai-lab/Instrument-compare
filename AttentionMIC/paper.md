# 0. 摘要

目前乐器声音的识别已经达到了很好的效果，但是相比单一乐器的音频流，真正的音乐都是多乐器混合的，这样复杂的音频环境给乐器的识别带来很大的挑战。用于多乐器的乐器识别的数据集可大体分为两种：第一种，比如MedleyDB，它有很强的逐帧数据的乐器活动的注解，但是这种数据集的规模都比较小，难以训练出鲁棒的模型；另外一种是规模比较大的数据集，比如像OpenMIC那样只有弱标签，在这里弱标签是说在一整段音乐音频中，某种乐器是否存在。我探索了一种基于注意力机制(Attention Mechanism)的深度学习方法来处理这种弱标签数据的多标签的乐器识别。我然后将这个基于注意力的模型与其他系统进行了比较，包括作为baseline的随机森林模型，循环神经网络模型(RNN)，以及全连接的神经网络。我的实验结果表面，基于注意力机制的神经网络模型对于OpenMIC数据的识别精度相比于其他模型有明显的提升。我发现注意力的策略可以使系统可以聚焦在和某个乐器标签相关的时间片上。

# 1. 绪论(Introduction)

因为最近十几年的计算架构的蓬勃发展，使得六七十年代就出现的人工神经网络在一些非结构数据的处理和分析上取得了惊人的效果，尤其是在图像，视频，音频，以及自然语言领域，效果尤为显著。虽然机器对图像，语音，自然语言的理解有了长足进步，但是音乐方面的应用并不常见。如果可以让计算机能够理解并欣赏音乐将是一件十分有趣的事情。另外随着信息技术的快速发展和互联网应用的普及,人们可以很便利地获取大量的音乐、视频等多媒体数据，基于内容的多媒体信息检索具有迫切的应用需求，因此乐器的识别对与音乐信息的检索分类是有重要意义的。

乐器分为电子类的和声学类的，他们都是创造音乐的必要工具。大多数音乐都是由许多不同音色特质的乐器混合而成的。我们人类当然相当擅长认出他们在音乐里听到的不一样的乐器。所以，自动识别乐器目前在音乐信息检索的学术界里是一个非常活跃研究领域。到目前为止，单一的乐器演奏已经达到了很好的效果，已经没有太多挑战性，然而从多种音乐混合演奏的音乐里识别出各种乐器还是一件很有挑战性的工作。它之所以这么难，主要有三个原因：

1. 多种乐器信息源的叠加(包括时间域和频率域)；
2. 同一种乐器的音色的变化都很大；
3. 对于有监督机器学习算法来说，缺乏代标签的数据集。

识别录音中的音乐有助于用户通过对某些特定的乐器来检索音乐信息。乐器的识别对于其他音乐信息检索方面的应用也很有帮助。比如，乐器标签也可以被音乐推荐系统用来给用户特定的偏好来建模，基于乐器信息标签的音乐风格识别系统也可以靠此来提升各项指标。基于可靠的乐器识别建立的条件概率模型，也会对诸如自动音乐转录，播放源分离，以及演奏技术检测等任务带来显著改善。

就像上文提到过的，音乐信息检索领域中的挑战之一就是缺乏大规模的标签数据来进行监督学习算法的训练，尤其是在乐器识别方面。基于复调音乐的乐器识别所用到的数据集大体可以分为两类：强标签数据和弱标签数据。弱标签数据集(Weakly Labeled
Dataset)是由一系列长达几秒，甚至十几秒的音乐片段组成，每一个片段的标签表明了在整个片段是否有一个或多个乐器的声音存在在片段里，而不是那种明确的标注了某个乐器开始和结束的具体时间；而强标签数据包含了细粒度的乐器演奏的活动统计，即某个乐器明确的开始和结束的时间。相比于强标签数据，弱标签数据更容易标注，因此也就更容易产生海量数据。尽管强标签数据可以很好的应用一些很强的有监督学习，但是更小的数据规模限制了深度学习算法的应用。然而弱标签数据也有个明显的缺点，即因为音乐片段很短，有可能某个乐器确实演奏了一小会儿，但是没有被标记。这也给弱标签数据的训练带来不小的挑战。

因为受到了强标签数据的启发，在弱标签中针对音频中相应乐器出现的具体时间的位置的推断将可能有助于提高模型的识别效果。因此我们将复调音乐的乐器识别问题转换为一个多实例多标签学习(Multi-Instance Multi-Label)的问题，每一个若标签数据都是好几个较短时间段的实例，每一个实例都对属于这个数据的标签有贡献。为此，我采用了一种注意力机制来汇聚每个较短时间的实例的预测，并且将这个方法于其他一些分类方法做比较，比如包括随机深林，全连接网络，以及循环神经网络(recurrent
neural networks)。

# 2. 业界相关工作

## 2.1 乐器识别

乐器识别的问题有一种略微简单的场景，即单乐器识别，通常是单个的音节或者一小段某个乐器的独奏。在单个的乐器识别中，基本都是先提取声学特征，比如MFCC或者LPCC等特征，然后应用一些基本的监督机器学习算法，比如最近邻算法(K-Nearest-Neighbour)，基于多类别的逻辑回归，和基于RBF核的支撑向量机算法。

而目前在乐器识别的研究基本都聚焦在复调音乐和多乐器合奏的录音上了。传统的特征提取加上分类算法的组合在之前是比较普遍，而最近深度神经网络的应用占到了主导位置。Yoonchang Han等人应用卷积神经网络在IRMAS数据集上，其性能超越了传统的基于特征提取的算法，另外还有人提出了利用卷积神经网络在MedleyDB数据集上直接在原始数据上训练参数。在后续研究中，在后续研究中，洪等人[18]提出了一种多任务学习方法用于乐器识别识别，除了对乐器的识别，他们还对音高进行了预测。他们发布了从MIDI文件合成的，大规模且强标记的数据集进行评估，发现采用多任务学习框架要比他们以前使用的以音高特征值作为附加输入的方法更好。

2.2 语音活动的检测，标注和分类

音频或者声音活动的分类和乐器识别有很多共同点。他们的目标都是从一段混合各种声音的音频流中识别出随时间变化的某一特定声源。但是其中有一些不同的是，声音事件的分类主要偏重于研究各种不相关的声音，比如，发动机噪音，车喇叭叫，婴儿哭声，或者狗吠，然而音乐的音频是高度相关的。另外，音乐具有丰富的和弦和时间结构，通常从真实世界的声音中捕获不到这样的声音。

# 3. 数据集介绍

# 4. 基于深度学习分类方法

在详细介绍深度学习的模型前，我们先要给我们的基于弱标签数据集的乐器识别方法提供一个形式化的介绍。

## 4.1 数据预处理

就像上一章介绍的一样，OpenMIC数据集是由十秒长的音频文件组成，每条数据的标签表示了二十种乐器中的一个或多个乐器是否存在在这条音乐片段中。对于数据集中的每一个音频文件，数据制作方还发布了一组用预先训练好的一个被称为"VGGish"的卷积神经网络提取出来的特征值。这个叫“VGGish”的模型是基于VGG架构的，VGG架构本来是用来在计算机视觉领域中做目标检测用的，而现在被训练出来做语音分类。这个模型基于音频文件0.96秒的时间窗口产生一个128维的特征向量，一个音频文件可以有十个不重叠的采样窗口。这些特征经过ZCA白化处理，并且量化到八位的整数，最后将这8位的整数做归一化处理，映射到0到1之间。

## 4.2 问题定义

### 4.2.1 多实例多标签问题(Multi-Instance Multi-Label Problem)

在一般情况下，乐器识别被定义为多标签多实例的分类问题。在这个设定下，假设数据集为 $ \{(X_1, Y_1), (X_2, Y_2), ... , (X_m, Y_m)\}$，其中的$X_i$可以被比喻为一个包含了r个实例的袋子，即

$$X_i = \{x_{i,1}, x_{i,2},...,x_{i,r} \}$$

还有

$$Y_i = [y_{i,1}, y_{i,2},...,y_{i,L}] \in \{0,1\}^L$$

$Y_i$是一个有L个标签的标签向量，如果$X_i$中的任何一个实例包含标签j，那么$y_{i,j}$就等于1。因为下标方式太繁琐，在余下的论文章节中，我将放弃这种方式来索引数据，我们就简单得用$(X,Y)$来表示数据集中的一条数据，一个X向量就是一个10 X 128维度的特征矩阵，它代表一段语音片段，且包含了十个实例。我们的这个问题同时也是一个标签遗失问题，因为数据集中不是所有的$y_j$是已知的。

在我们的实验里，假设所有标签都可以针对每个实例进行独立得预测。在这个假设下，我们的多实例多标签问题可以分解为L(在openmic数据集当中，L等于20)个实例的多实例学习问题，每一个实例对应数据集中的每个标签。

请注意，在多标签分类问题中利用标签的相关性信息已经被证明可以显著得提高。然而在基于openmic数据集的乐器识别问题上采用这种结合了标签相关性的方法会带来额外的挑战：即标签缺失和稀疏标签的问题。

### 4.2.2 多实例学习(Multi-Instance Learning)

在多示例学习问题的设定下，一个袋子的标签是通过一个目标函数$S(x)$得到的。在示例之间独立的假设下，　$S(X)$ 可以表示为一种参数形式：

$$S(X) = \mu(f(x)) \tag{1}$$

在上面公式里，$f(.)$是对一个单个示例$x$的目标函数，$\mu(.)$是对示例目标函数$f(x)$的一个排列不变的聚合操作。这个参数化公式很自然得可以得到一个分类一个数据包内所有示例的方法：

1. 用单个的示例目标函数为每一个示例计算出一个目标值；
2. 用聚合函数$\mu(.)$将示例的目标值做一个累加；

在我们的方法中，我们使用一个分类函数来对每个示例产生目标一个分数$f(x)$，这个分数的数值基本上是一个标签是否会在一个示例中出现的概率。max和avg这两个函数是大家常用的聚合函数(将数据包中的每个示例的分数聚合为一个整体)，当然他们都具有排列不变的性质。然而我们选择了可训练参数的加权操作来作为我们的聚合操作：

$$S(X) = \sum\limits_{x \in X} \omega_x f(x) \tag{2}$$

在上式中，$\omega_x$是一个相对于一个示例$x$的可训练的参数。我们对于$f(.)$和$\mu(.)$的选择有两个优势：

1. $S(.)$函数的结果就是一个标签出现在数据包中的概率，并且可以直接用来进行预测。
2. 为每一个示例训练出的参数$\omega_x$通过多示例模型为每个示例的目标分数来编码的置信参数来为这个模型增加了更多的解释性。

### 4.2.3 注意力机制

这个参数可训练的聚合操作等价于注意力机制(Attention Mechanism)。假设有一个数据包里面由$r$个示例：$X=\{x_1,...,x_r\}$，再通过基于单示例的目标函数$f(.)$生成了一个新的数据包：$\{ f(x_1),...,f(x_r) \}$，它代表所有$r$个示例的目标分数。然后整体的目标分数$S(X)$就可以通过公式(2)计算出来。

我们进一步对示例的权重$\omega_x$增加限制：让它们加起来为1，数学上的表达是$\sum_{x \in X}\omega_x=1$。这样就确保了聚合操作相对于数据包里示例的数量是不变的，从而使得我们的模型可以被应用在任意长度的声音片段上。此外，这种归一化(Normalization)将带来每个示例的权重的概率表达，它可以用来推断每个示例对于总体目标$S(X)$的相对贡献。假如有一个示例$x \in X$，其权重$\omega_x$可以被以下的公式来参数化：

$$
\omega_x=\frac{\sigma(\upsilon^Th(x))}{\sum_{x^\prime \in X} \sigma(\upsilon^Th(x^\prime))}  \tag{3}
$$

在上述公式中，$h(x)$是一个可训练的基于示例$x$的嵌入层(Embedding Layer)，$\upsilon$是注意力层(Attention Layer)的一些可训练的参数 向量，最后$\sigma(.)$是非线性的sigmoid函数。

## 4.3 模型架构

为了计算整个数据包的目标分数$S(.)$，我们首先需要用$f(.)$来计算每个示例的分数，然后通过一个可训练的操作$\mu(.)$来将每个示例的分数聚合起来，其中的权值$\omega_x$利用公式(3)来计算。对于我们的实验而言，包括数据包级别的目标函数$S(.)$, 和示例级别的目标函数$f(.)$，它们被表达为数据包级别的或者单个示例级别的，对于某个标签而言是否是一个正向的标本数据的概率估计。

总体的结构流程是：我们首先将每一个示例数据$x$输入到一个由三层全连接网络层(Fully Connected Layer)组成的嵌入层里。然后将输出的每一个示例的值带入$f(.)$

## 4.4 损失函数与训练过程

# 5. 分类结果的统计分析

